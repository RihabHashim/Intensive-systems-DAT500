{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement Regression algorithm without using MLlib\n",
    "from pyspark import SparkConf,SparkContext\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import broadcast\n",
    "params_num = 1000\n",
    "mean = 0\n",
    "std = 4\n",
    "loss_list = []\n",
    "for i in range(50):\n",
    "   params = np.random.normal(mean, std, [para9ms_num, 10])\n",
    "   a = np.array(range(0,params_num))[...,None]\\\n",
    "   params = np.hstack((params, a))\n",
    "   rdd1 = sc.parallelize(params)\n",
    "   rdd2 = rdd1.map(lambda x: [float(i) for i in x])\n",
    "   params_df = rdd2.toDF([\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\",\"H\",\"I\",\"J\", \"id\"])\n",
    "   data_df = spark.read.parquet(\"/user/s_feeds/Rihab/dis_materials/fast1_metadata.parquet\")\n",
    "   mapping = {col: col.replace('.','_').replace(' ', '_') for col in data_df.columns}\n",
    "   from pyspark.sql.functions import col\n",
    "   data_df = data_df.select([col('`'+c+'`').alias(mapping.get(c, c)) for c in data_df.columns])\n",
    "   from pyspark.sql.functions import asc,abs, row_number, monotonically_increasing_id\n",
    "   from pyspark.sql.window import Window\n",
    "   data_df = data_df.select(\"out_electricity_total_energy_consumption\", \"in_sqft\",\n",
    "            \"in_number_of_stories\").withColumn(\"random_id\",(row_number().over\n",
    "              (Window.orderBy(monotonically_increasing_id())) - 1)% params_num)\n",
    "   df = data_df.join(broadcast(params_df),data_df.random_id==params_df.id)\n",
    "   df = df.withColumn(\"loss\", abs(col(\"out_electricity_total_energy_consumption\") - \n",
    "             col('A') * col(\"in_sqft\") ** 3 - col('B') * col(\"in_sqft\") ** 2 * \n",
    "             col(\"in_number_of_stories\") - col('C') * col(\"in_sqft\") * \n",
    "             col(\"in_number_of_stories\") ** 2  - col('D')  * col(\"in_number_of_stories\")\n",
    "             ** 3 - col('E') * col(\"in_sqft\") ** 2 -  col('F') * col(\"in_sqft\") * \n",
    "             col(\"in_number_of_stories\") -  col('G') * col(\"in_number_of_stories\") **\n",
    "             2 -  col('H') * col(\"in_sqft\") -  col('I') * col(\"in_number_of_stories\") -  col('J')))\n",
    "   from pyspark.sql.functions import mean\n",
    "   result = df.groupBy(\"id\").agg(mean('loss').alias(\"mean\")).sort(asc(\"mean\"))\n",
    "   r = result.head(100)\n",
    "   print(r[0])\n",
    "   loss_list.append(\"step\" +str(i) +\" loss:\" + str(r[0].mean))\n",
    "   l = [int(i.id) for i in r]\n",
    "   params_id = np.take(params, l, axis=0)\n",
    "   new_params = params_id[:, :-1]\n",
    "   mean = new_params.mean(0)\n",
    "   std = new_params.std(0)\n",
    "   print(\"step=\", i, \"mean=\", mean, \"std=\", std)\n",
    "\n",
    "################################evaluate running time##############\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import broadcast\n",
    "import time\n",
    "params_num = 1000\n",
    "mean = 0\n",
    "std = 4\n",
    "loss_list = []\n",
    "start_time = time.time()\n",
    "round = 2\n",
    "for i in range(round):\n",
    "        params = np.random.normal(mean, std, [params_num, 10])\n",
    "        a = np.array(range(0,params_num))[...,None]\n",
    "        params = np.hstack((params, a))\n",
    "        rdd1 = sc.parallelize(params)\n",
    "        rdd2 = rdd1.map(lambda x: [float(i) for i in x])\n",
    "        params_df = rdd2.toDF([\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\",\"H\",\"I\",\"J\", \"id\"])\n",
    "        data_df = spark.read.parquet(\"hdfs://namenode:9000/dis_materials/fast2_metadata.parquet\")\n",
    "        mapping = {col: col.replace('.','_').replace(' ', '_') for col in data_df.columns}\n",
    "        from pyspark.sql.functions import col\n",
    "        data_df = data_df.select([col('`'+c+'`').alias(mapping.get(c, c)) for c in data_df.columns])\n",
    "from pyspark.sql.functions import asc,abs, row_number, monotonically_increasing_id\n",
    "from pyspark.sql.window import Window \n",
    "data_df = data_df.select(\"out_electricity_total_energy_consumption\", \"in_sqft\", \"in_number_of_stories\").\n",
    "withColumn(\"random_id\",(row_number().over(Window.orderBy(monotonically_increasing_id())) - 1)% params_num)\n",
    "df = data_df.join(broadcast(params_df),data_df.random_id==params_df.id)\n",
    "df = df.withColumn(\"loss\", abs(col(\"out_electricity_total_energy_consumption\") -\n",
    "        col('A') * col(\"in_sqft\") ** 3 - col('B') * col(\"in_sqft\") ** 2 * \n",
    "        col(\"in_number_of_stories\") - col('C') * col(\"in_sqft\") * col(\"in_number_of_stories\")\n",
    "        ** 2  - col('D')  * col(\"in_number_of_stories\") ** 3 - col('E') * \n",
    "        col(\"in_sqft\") ** 2 -  col('F') * col(\"in_sqft\") * col(\"in_number_of_stories\") - \n",
    "        col('G') * col(\"in_number_of_stories\") ** 2 -  col('H') * col(\"in_sqft\") -  col('I')\n",
    "        * col(\"in_number_of_stories\") -  col('J')))\n",
    "from pyspark.sql.functions import mean \n",
    "result = df.groupBy(\"id\").agg(mean('loss').alias(\"mean\")).sort(asc(\"mean\"))\n",
    "r = result.head(100) \n",
    "print(r[0])\n",
    "loss_list.append(\"step\" +str(i) +\" loss:\" + str(r[0].mean))\n",
    "l = [int(i.id) for i in r]\n",
    "params_id = np.take(params, l, axis=0)\n",
    "new_params = params_id[:, :-1]   \n",
    "mean = new_params.mean(0)  \n",
    "std = new_params.std(0) \n",
    "print(\"step=\", i, \"mean=\", mean, \"std=\", std)\n",
    "print(str(round)+\" round time cost:\" + str(time.time() - start_time))\n",
    "print(loss_list) "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
